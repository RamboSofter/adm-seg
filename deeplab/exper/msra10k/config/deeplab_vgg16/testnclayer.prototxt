# VGG 16-layer network convolutional finetuning
# Network modified to have smaller receptive field (128 pixels)
# and smaller stride (8 pixels) when run in convolutional mode.
#
# In this model we also change max pooling size in the first 4 layers
# from 2 to 3 while retaining stride = 2
# which makes it easier to exactly align responses at different layers.
#

name: "${NET_ID}"

layer {
  name: "data"
  type: "ImageSegData"
  top: "data"
  top: "label"
  top: "data_dim"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 513
    mean_value: 104.008
    mean_value: 116.669
    mean_value: 122.675
  }
  image_data_param {
    root_folder: "${DATA_ROOT}"
    source: "${EXP}/list/${TEST_SET}.txt"
    batch_size: 1
    label_type: PIXEL
  }
}

###################### DeepLab ##########################
layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3_3"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu4_3"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
layer {
  bottom: "conv4_3"
  top: "pool4"
  name: "pool4"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 3
    pad: 1
    stride: 1
  }
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "pool4"
  top: "conv5_1"
  convolution_param {
    num_output: 512
    pad: 2
    kernel_size: 3
    dilation: 2
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  convolution_param {
    num_output: 512
    pad: 2
    kernel_size: 3
    dilation: 2
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  convolution_param {
    num_output: 512
    pad: 2
    kernel_size: 3
    dilation: 2
  }
}
layer {
  name: "relu5_3"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  bottom: "conv5_3"
  top: "pool5"
  name: "pool5"
  type: "Pooling"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
    pad: 1
  }
}

### hole = 6
layer {
  name: "fc6_1"
  type: "Convolution"
  bottom: "pool5"
  top: "fc6_1"
  convolution_param {
    num_output: 1024
    pad: 6
    kernel_size: 3
    dilation: 6
  }
}
layer {
  name: "relu6_1"
  type: "ReLU"
  bottom: "fc6_1"
  top: "fc6_1"
}
layer {
  name: "drop6_1"
  type: "Dropout"
  bottom: "fc6_1"
  top: "fc6_1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_1"
  type: "Convolution"
  bottom: "fc6_1"
  top: "fc7_1"
  convolution_param {
    num_output: 1024
    kernel_size: 1
  }
}
layer {
  name: "relu7_1"
  type: "ReLU"
  bottom: "fc7_1"
  top: "fc7_1"
}
layer {
  name: "drop7_1"
  type: "Dropout"
  bottom: "fc7_1"
  top: "fc7_1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_${EXP}_1"
  type: "Convolution"
  bottom: "fc7_1"
  top: "fc8_${EXP}_1"
  convolution_param {
    num_output: ${NUM_LABELS}
    kernel_size: 1
  }
}

### hole = 12
layer {
  name: "fc6_2"
  type: "Convolution"
  bottom: "pool5"
  top: "fc6_2"
  convolution_param {
    num_output: 1024
    pad: 12
    kernel_size: 3
    dilation: 12
  }
}
layer {
  name: "relu6_2"
  type: "ReLU"
  bottom: "fc6_2"
  top: "fc6_2"
}
layer {
  name: "drop6_2"
  type: "Dropout"
  bottom: "fc6_2"
  top: "fc6_2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_2"
  type: "Convolution"
  bottom: "fc6_2"
  top: "fc7_2"
  convolution_param {
    num_output: 1024
    kernel_size: 1
  }
}
layer {
  name: "relu7_2"
  type: "ReLU"
  bottom: "fc7_2"
  top: "fc7_2"
}
layer {
  name: "drop7_2"
  type: "Dropout"
  bottom: "fc7_2"
  top: "fc7_2"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_${EXP}_2"
  type: "Convolution"
  bottom: "fc7_2"
  top: "fc8_${EXP}_2"
  convolution_param {
    num_output: ${NUM_LABELS}
    kernel_size: 1
  }
}

### hole = 18
layer {
  name: "fc6_3"
  type: "Convolution"
  bottom: "pool5"
  top: "fc6_3"
  convolution_param {
    num_output: 1024
    pad: 18
    kernel_size: 3
    dilation: 18
  }
}
layer {
  name: "relu6_3"
  type: "ReLU"
  bottom: "fc6_3"
  top: "fc6_3"
}
layer {
  name: "drop6_3"
  type: "Dropout"
  bottom: "fc6_3"
  top: "fc6_3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_3"
  type: "Convolution"
  bottom: "fc6_3"
  top: "fc7_3"
  convolution_param {
    num_output: 1024
    kernel_size: 1
  }
}
layer {
  name: "relu7_3"
  type: "ReLU"
  bottom: "fc7_3"
  top: "fc7_3"
}
layer {
  name: "drop7_3"
  type: "Dropout"
  bottom: "fc7_3"
  top: "fc7_3"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_${EXP}_3"
  type: "Convolution"
  bottom: "fc7_3"
  top: "fc8_${EXP}_3"
  convolution_param {
    num_output: ${NUM_LABELS}
    kernel_size: 1
  }
}

### hole = 24
layer {
  name: "fc6_4"
  type: "Convolution"
  bottom: "pool5"
  top: "fc6_4"
  convolution_param {
    num_output: 1024
    pad: 24
    kernel_size: 3
    dilation: 24
  }
}
layer {
  name: "relu6_4"
  type: "ReLU"
  bottom: "fc6_4"
  top: "fc6_4"
}
layer {
  name: "drop6_4"
  type: "Dropout"
  bottom: "fc6_4"
  top: "fc6_4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7_4"
  type: "Convolution"
  bottom: "fc6_4"
  top: "fc7_4"
  convolution_param {
    num_output: 1024
    kernel_size: 1
  }
}
layer {
  name: "relu7_4"
  type: "ReLU"
  bottom: "fc7_4"
  top: "fc7_4"
}
layer {
  name: "drop7_4"
  type: "Dropout"
  bottom: "fc7_4"
  top: "fc7_4"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8_${EXP}_4"
  type: "Convolution"
  bottom: "fc7_4"
  top: "fc8_${EXP}_4"
  convolution_param {
    num_output: ${NUM_LABELS}
    kernel_size: 1
  }
}

### SUM the four branches
layer {
  bottom: "fc8_${EXP}_1"
  bottom: "fc8_${EXP}_2"
  bottom: "fc8_${EXP}_3"
  bottom: "fc8_${EXP}_4"
  top: "fc8_${EXP}"
  name: "fc8_${EXP}"
  type: "Eltwise"
  eltwise_param {
    operation: SUM
  }
}
## original resolution
layer {
  name: "fc8_interp"
  type: "Interp"
  bottom: "fc8_${EXP}"
  top: "fc8_interp"
  interp_param {
    zoom_factor: 8
  }
}
# layer {
#   bottom: "fc8_interp"
#   top: "fc8_interp_argmax"
#   name: "fc8_interp_argmax"
#   type: "ArgMax"
#   argmax_param {
#     axis: 1
#   }
# }
layer {
  bottom: "fc8_interp"
  top: "fc8_interp_softmax"
  name: "fc8_interp_softmax"
  type: "Softmax"
}
#layer {
#  name: "fc8_mat"
#  type: "MatWrite"
#  bottom: "fc8_interp_softmax"
#  include {
#    phase: TEST
#  }
#  mat_write_param {
#    prefix: "${FEATURE_DIR}/${TEST_SET}/fc8/"
#    source: "${EXP}/list/${TEST_SET}_id.txt"
#    strip: 0
#    period: 1
#  }
#}
#layer {
#  bottom: "data"
#  bottom: "fc8_interp_softmax"
#  bottom: "label"
#  top: "nc_loss"
#  name: "nc_loss"
#  type: "NormalizedCut"
#  normalized_cut_param {
#    bi_xy_std: 40
#    bi_rgb_std: 15
#    all_labels: true
#  }
#}

layer {
  bottom: "label"
  top: "label_shrink"
  name: "label_shrink"
  type: "Interp"
  interp_param {
    shrink_factor: 8
    pad_beg: 0
    pad_end: 0
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_${EXP}"
  bottom: "label_shrink"
  top: "softmax_loss"
  include {
    phase: TEST
  }
  loss_param {
    ignore_label: 255
  }
}

layer {
  name: "accuracy"
  type: "SegAccuracy"
  bottom: "fc8_interp"
  bottom: "label"
  top: "accuracy"
  seg_accuracy_param {
    ignore_label: 255
  }
}

############################## NC layers begin ################################
layer {
  bottom: "fc8_interp_softmax"
  top: "logp"
  name: "logp"
  type: "Log"
}
######################### first iteration
layer {
  bottom: "data"
  bottom: "fc8_interp_softmax"
  propagate_down: false
  propagate_down: true
  top: "nc_bound1"
  name: "nc_bound1"
  type: "NormalizedCutBound"
  loss_param {
    ignore_label: 255
  }
  normalized_cut_param {
    bi_xy_std: ${BI_XY_STD}
    bi_rgb_std: ${BI_RGB_STD}
    bi_weight: ${BI_WEIGHT}
  }
}

layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "logp"
  bottom: "nc_bound1"
  top: "logp_nc_bound1"
  eltwise_param { 
    operation: SUM
    coeff: ${ELTWISE_SCALING}
    coeff: -${ELTWISE_SCALING}
  }
}

layer {
  bottom: "logp_nc_bound1"
  top: "Q1"
  name: "Q1"
  type: "Softmax"
}

layer {
  name: "ncaccuracy1"
  type: "SegAccuracy"
  bottom: "Q1"
  bottom: "label"
  top: "ncaccuracy1"
  seg_accuracy_param {
    ignore_label: 255
  }
}

################## second iteration
layer {
  bottom: "data"
  bottom: "Q1"
  propagate_down: false
  propagate_down: true
  top: "nc_bound2"
  name: "nc_bound2"
  type: "NormalizedCutBound"
  loss_param {
    ignore_label: 255
  }
  normalized_cut_param {
    bi_xy_std: ${BI_XY_STD}
    bi_rgb_std: ${BI_RGB_STD}
    bi_weight: ${BI_WEIGHT}
  }
}

layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "logp"
  bottom: "nc_bound2"
  top: "logp_nc_bound2"
  eltwise_param { 
    operation: SUM
    coeff: ${ELTWISE_SCALING}
    coeff: -${ELTWISE_SCALING}
  }
}

layer {
  bottom: "logp_nc_bound2"
  top: "Q2"
  name: "Q2"
  type: "Softmax"
}

layer {
  name: "ncaccuracy2"
  type: "SegAccuracy"
  bottom: "Q2"
  bottom: "label"
  top: "ncaccuracy2"
  seg_accuracy_param {
    ignore_label: 255
  }
}

################## third iteration
layer {
  bottom: "data"
  bottom: "Q2"
  propagate_down: false
  propagate_down: true
  top: "nc_bound3"
  name: "nc_bound3"
  type: "NormalizedCutBound"
  loss_param {
    ignore_label: 255
  }
  normalized_cut_param {
    bi_xy_std: ${BI_XY_STD}
    bi_rgb_std: ${BI_RGB_STD}
    bi_weight: ${BI_WEIGHT}
  }
}

layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "logp"
  bottom: "nc_bound3"
  top: "logp_nc_bound3"
  eltwise_param { 
    operation: SUM
    coeff: ${ELTWISE_SCALING}
    coeff: -${ELTWISE_SCALING}
  }
}

layer {
  bottom: "logp_nc_bound3"
  top: "Q3"
  name: "Q3"
  type: "Softmax"
}

layer {
  name: "ncaccuracy3"
  type: "SegAccuracy"
  bottom: "Q3"
  bottom: "label"
  top: "ncaccuracy3"
  seg_accuracy_param {
    ignore_label: 255
  }
}

################## fourth iteration
layer {
  bottom: "data"
  bottom: "Q3"
  propagate_down: false
  propagate_down: true
  top: "nc_bound4"
  name: "nc_bound4"
  type: "NormalizedCutBound"
  loss_param {
    ignore_label: 255
  }
  normalized_cut_param {
    bi_xy_std: ${BI_XY_STD}
    bi_rgb_std: ${BI_RGB_STD}
    bi_weight: ${BI_WEIGHT}
  }
}

layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "logp"
  bottom: "nc_bound4"
  top: "logp_nc_bound4"
  eltwise_param { 
    operation: SUM
    coeff: ${ELTWISE_SCALING}
    coeff: -${ELTWISE_SCALING}
  }
}

layer {
  bottom: "logp_nc_bound4"
  top: "Q4"
  name: "Q4"
  type: "Softmax"
}

layer {
  name: "ncaccuracy4"
  type: "SegAccuracy"
  bottom: "Q4"
  bottom: "label"
  top: "ncaccuracy4"
  seg_accuracy_param {
    ignore_label: 255
  }
}

################## fifth iteration
layer {
  bottom: "data"
  bottom: "Q4"
  propagate_down: false
  propagate_down: true
  top: "nc_bound5"
  name: "nc_bound5"
  type: "NormalizedCutBound"
  loss_param {
    ignore_label: 255
  }
  normalized_cut_param {
    bi_xy_std: ${BI_XY_STD}
    bi_rgb_std: ${BI_RGB_STD}
    bi_weight: ${BI_WEIGHT}
  }
}

layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "logp"
  bottom: "nc_bound5"
  top: "logp_nc_bound5"
  eltwise_param { 
    operation: SUM
    coeff: ${ELTWISE_SCALING}
    coeff: -${ELTWISE_SCALING}
  }
}

layer {
  bottom: "logp_nc_bound5"
  top: "Q5"
  name: "Q5"
  type: "Softmax"
}

layer {
  name: "ncaccuracy5"
  type: "SegAccuracy"
  bottom: "Q5"
  bottom: "label"
  top: "ncaccuracy5"
  seg_accuracy_param {
    ignore_label: 255
  }
}


################## sixth iteration
layer {
  bottom: "data"
  bottom: "Q5"
  propagate_down: false
  propagate_down: true
  top: "nc_bound6"
  name: "nc_bound6"
  type: "NormalizedCutBound"
  loss_param {
    ignore_label: 255
  }
  normalized_cut_param {
    bi_xy_std: ${BI_XY_STD}
    bi_rgb_std: ${BI_RGB_STD}
    bi_weight: ${BI_WEIGHT}
  }
}

layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "logp"
  bottom: "nc_bound6"
  top: "logp_nc_bound6"
  eltwise_param { 
    operation: SUM
    coeff: ${ELTWISE_SCALING}
    coeff: -${ELTWISE_SCALING}
  }
}

layer {
  bottom: "logp_nc_bound6"
  top: "Q6"
  name: "Q6"
  type: "Softmax"
}

layer {
  name: "ncaccuracy6"
  type: "SegAccuracy"
  bottom: "Q6"
  bottom: "label"
  top: "ncaccuracy6"
  seg_accuracy_param {
    ignore_label: 255
  }
}

################## seventh iteration
layer {
  bottom: "data"
  bottom: "Q6"
  propagate_down: false
  propagate_down: true
  top: "nc_bound7"
  name: "nc_bound7"
  type: "NormalizedCutBound"
  loss_param {
    ignore_label: 255
  }
  normalized_cut_param {
    bi_xy_std: ${BI_XY_STD}
    bi_rgb_std: ${BI_RGB_STD}
    bi_weight: ${BI_WEIGHT}
  }
}

layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "logp"
  bottom: "nc_bound7"
  top: "logp_nc_bound7"
  eltwise_param { 
    operation: SUM
    coeff: ${ELTWISE_SCALING}
    coeff: -${ELTWISE_SCALING}
  }
}

layer {
  bottom: "logp_nc_bound7"
  top: "Q7"
  name: "Q7"
  type: "Softmax"
}

layer {
  name: "ncaccuracy7"
  type: "SegAccuracy"
  bottom: "Q7"
  bottom: "label"
  top: "ncaccuracy7"
  seg_accuracy_param {
    ignore_label: 255
  }
}


################## eighth iteration
layer {
  bottom: "data"
  bottom: "Q7"
  propagate_down: false
  propagate_down: true
  top: "nc_bound8"
  name: "nc_bound8"
  type: "NormalizedCutBound"
  loss_param {
    ignore_label: 255
  }
  normalized_cut_param {
    bi_xy_std: ${BI_XY_STD}
    bi_rgb_std: ${BI_RGB_STD}
    bi_weight: ${BI_WEIGHT}
  }
}

layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "logp"
  bottom: "nc_bound8"
  top: "logp_nc_bound8"
  eltwise_param { 
    operation: SUM
    coeff: ${ELTWISE_SCALING}
    coeff: -${ELTWISE_SCALING}
  }
}

layer {
  bottom: "logp_nc_bound8"
  top: "Q8"
  name: "Q8"
  type: "Softmax"
}

layer {
  name: "ncaccuracy8"
  type: "SegAccuracy"
  bottom: "Q8"
  bottom: "label"
  top: "ncaccuracy8"
  seg_accuracy_param {
    ignore_label: 255
  }
}

################## nineth iteration
layer {
  bottom: "data"
  bottom: "Q8"
  propagate_down: false
  propagate_down: true
  top: "nc_bound9"
  name: "nc_bound9"
  type: "NormalizedCutBound"
  loss_param {
    ignore_label: 255
  }
  normalized_cut_param {
    bi_xy_std: ${BI_XY_STD}
    bi_rgb_std: ${BI_RGB_STD}
    bi_weight: ${BI_WEIGHT}
  }
}

layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "logp"
  bottom: "nc_bound9"
  top: "logp_nc_bound9"
  eltwise_param { 
    operation: SUM
    coeff: ${ELTWISE_SCALING}
    coeff: -${ELTWISE_SCALING}
  }
}

layer {
  bottom: "logp_nc_bound9"
  top: "Q9"
  name: "Q9"
  type: "Softmax"
}

layer {
  name: "ncaccuracy9"
  type: "SegAccuracy"
  bottom: "Q9"
  bottom: "label"
  top: "ncaccuracy9"
  seg_accuracy_param {
    ignore_label: 255
  }
}

################## tenth iteration
layer {
  bottom: "data"
  bottom: "Q9"
  propagate_down: false
  propagate_down: true
  top: "nc_bound10"
  name: "nc_bound10"
  type: "NormalizedCutBound"
  loss_param {
    ignore_label: 255
  }
  normalized_cut_param {
    bi_xy_std: ${BI_XY_STD}
    bi_rgb_std: ${BI_RGB_STD}
    bi_weight: ${BI_WEIGHT}
  }
}

layer {
  name: "eltwise-sum"
  type: "Eltwise"
  bottom: "logp"
  bottom: "nc_bound10"
  top: "logp_nc_bound10"
  eltwise_param { 
    operation: SUM
    coeff: ${ELTWISE_SCALING}
    coeff: -${ELTWISE_SCALING}
  }
}

layer {
  bottom: "logp_nc_bound10"
  top: "Q10"
  name: "Q10"
  type: "Softmax"
}

layer {
  name: "ncaccuracy10"
  type: "SegAccuracy"
  bottom: "Q10"
  bottom: "label"
  top: "ncaccuracy10"
  seg_accuracy_param {
    ignore_label: 255
  }
}
######################## NC layers end ##################################


layer {
  name: "silence"
  type: "Silence"
  bottom: "label"
  bottom: "data_dim"
  bottom: "label_shrink"
  bottom: "Q1"
  bottom: "Q2"
  bottom: "Q3"
  bottom: "Q4"
  bottom: "Q5"
  bottom: "Q6"
  bottom: "Q7"
  bottom: "Q8"
  bottom: "Q9"
  bottom: "Q10"
}
